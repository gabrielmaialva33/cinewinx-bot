import logging
import re
import unicodedata
from math import ceil

import asyncio
from pyrogram import filters, Client
from pyrogram.errors import FloodWait
from pyrogram.types import (
    Message,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
    CallbackQuery,
)

from CineWinx import app
from CineWinx.utils import SessionAsyncClient
from config import PREFIXES, BANNED_USERS, LX_CHT_MODELS
from strings import get_command

LLM_COMMAND = get_command("LLM_COMMAND")

context_db: dict = {}

main_prompt = (
    "VocÃª Ã© a AI do CineWinx. Ao responder, por favor, chame o usuÃ¡rio pelo nome. {0}\n\n"
    "A seguir estÃ¡ o prompt:\n\n{1}"
)


@app.on_message(filters.command(LLM_COMMAND, PREFIXES) & ~BANNED_USERS)
async def llm(_client: Client, message: Message):
    prompt = get_prompt(message)
    if prompt is None:
        return await message.reply_text("ğŸ¦™ ğ˜ƒğ—¼ğ—°ğ—²Ì‚ ğ—»ğ—®Ìƒğ—¼ ğ—ºğ—² ğ—±ğ—²ğ˜‚ ğ˜‚ğ—º ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜!")

    reply_to_id = (
        message.reply_to_message.id if message.reply_to_message else message.id
    )

    user_name = message.from_user.first_name
    user_name = normalize_username(user_name)
    if user_name == "":
        user_name = "UsuÃ¡rio"

    context_db[message.from_user.id] = {
        "prompt": prompt,
        "reply_to_id": reply_to_id,
        "user_name": user_name,
        "model_id": None,
        "model_name": None,
    }

    markup = chat_models_markup(message.from_user.id, LX_CHT_MODELS)

    await message.reply_text(
        f"ğŸ¦™ ğ—¦ğ—²ğ—¹ğ—²ğ—°ğ—¶ğ—¼ğ—»ğ—² ğ˜‚ğ—º ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ—¼ ğ—Ÿğ—Ÿğ—  ğŸ‘‡",
        reply_markup=markup,
    )


def chat_models_markup(
        user_id: int, models: list | dict, page: int = 1
) -> InlineKeyboardMarkup:
    models = sorted(
        [
            InlineKeyboardButton(
                model["name"],
                callback_data=f"llm_{user_id}_{model['id']}_{model['name']}",
            )
            for model in models
        ],
        key=lambda x: x.text,
    )

    pairs = list(zip(models[::2], models[1::2]))
    i = 0
    for m in pairs:
        for _ in m:
            i += 1
    if len(models) - i == 1:
        pairs.append((models[-1],))
    elif len(models) - i == 2:
        pairs.append((models[-2], models[-1]))

    column_size = 3
    max_num_pages = ceil(len(pairs) / column_size)
    modulo_page = page % max_num_pages

    if len(pairs) > column_size:
        pairs = pairs[modulo_page * column_size: column_size * (modulo_page + 1)] + [
            (
                InlineKeyboardButton(
                    "â¬…ï¸ ğ—”ğ—»ğ˜ğ—²ğ—¿ğ—¶ğ—¼ğ—¿", callback_data=f"llm_prev_{modulo_page}"
                ),
                InlineKeyboardButton("âŒ", callback_data=f"llm_cancel_{user_id}"),
                InlineKeyboardButton(
                    "â¡ï¸ ğ—£ğ—¿ğ—¼Ìğ˜…ğ—¶ğ—ºğ—¼", callback_data=f"llm_next_{modulo_page}"
                ),
            )
        ]
    else:
        pairs += [
            [
                InlineKeyboardButton(
                    "âŒ ğ—–ğ—®ğ—»ğ—°ğ—²ğ—¹ğ—®ğ—¿", callback_data=f"llm_cancel_{user_id}"
                )
            ]
        ]

    return InlineKeyboardMarkup(pairs)


@app.on_callback_query(filters.regex(pattern=r"^llm_(prev|next)_\d+"))
async def paginate_models(_: Client, callback_query: CallbackQuery):
    try:
        data = callback_query.data.split("_")
        page = int(data[2])

        if data[1] == "prev":
            page -= 1
        elif data[1] == "next":
            page += 1

        markup = chat_models_markup(callback_query.from_user.id, LX_CHT_MODELS, page)

        await callback_query.edit_message_reply_markup(markup)
    except FloodWait as e:
        logging.warning(e)
        await asyncio.sleep(e.value)
    except Exception as e:
        logging.warning(e)


@app.on_callback_query(filters.regex(pattern=r"^llm_\d+_\d+_\w+") & ~BANNED_USERS)
async def select_model(_: Client, callback_query: CallbackQuery):
    user_id = int(callback_query.data.split("_")[1])
    model_id = int(callback_query.data.split("_")[2])
    model_name = callback_query.data.split("_")[3]

    # check if the user is the same as the one who initiated the command
    if callback_query.from_user.id != user_id:
        return await callback_query.answer("âŒ ğ—¡Ã£ğ—¼ ğ—®ğ˜‚ğ˜ğ—¼ğ—¿ğ—¶ğ˜‡ğ—®ğ—±ğ—¼.", show_alert=True)

    context_db[user_id]["model_id"] = model_id
    context_db[user_id]["model_name"] = model_name

    prompt = main_prompt.format(
        context_db[user_id]["user_name"], context_db[user_id]["prompt"]
    )

    params = {
        "prompt": prompt,
        "model_id": context_db[user_id]["model_id"],
    }

    query = await callback_query.message.edit(text="ğŸ”„ ğ—šğ—²ğ—¿ğ—®ğ—»ğ—±ğ—¼ ...", reply_markup=None)

    try:
        async with SessionAsyncClient() as client_async:
            response = await client_async.fetch(
                url=f"{client_async.url}/models",
                method="POST",
                params=params,
                json={},
                headers={"content-type": "application/json"},
            )
            if response["code"] != 2:
                return await query.edit_text(
                    f"ğŸ¦™ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼: {context_db[user_id]['model_name']}\n"
                    f"âŒ ğ—”ğ—¹ğ—´ğ—¼ ğ—±ğ—²ğ˜‚ ğ—²ğ—¿ğ—¿ğ—¼, ğ˜ğ—²ğ—»ğ˜ğ—² ğ—»ğ—¼ğ˜ƒğ—®ğ—ºğ—²ğ—»ğ˜ğ—² ğ—ºğ—®ğ—¶ğ˜€ ğ˜ğ—®ğ—¿ğ—±ğ—²."
                )

            await query.edit_text(
                f"ğŸ“ <u>ğ—£ğ—¿ğ—¼ğ—ºğ—½ğ˜</u>: {context_db[user_id]['prompt']}"
                f"\nğŸ¦™ <u>ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼</u>: {context_db[user_id]['model_name']}"
                f"\nğŸ“¬ <u>ğ—¥ğ—²ğ˜€ğ—½ğ—¼ğ˜€ğ˜ğ—®</u>: \n\n"
                f"<i>{response['content']}</i>",
            )
    except ValueError as e:
        logging.warning(e)
        return await query.edit_text(
            f"ğŸ¦™ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼: {context_db[user_id]['model_name']}\n"
            f"âŒ ğ—”ğ—¹ğ—´ğ—¼ ğ—±ğ—²ğ˜‚ ğ—²ğ—¿ğ—¿ğ—¼, ğ˜ğ—²ğ—»ğ˜ğ—² ğ—»ğ—¼ğ˜ƒğ—®ğ—ºğ—²ğ—»ğ˜ğ—² ğ—ºğ—®ğ—¶ğ˜€ ğ˜ğ—®ğ—¿ğ—±ğ—²."
        )
    except Exception as e:
        logging.warning(e)
        return await query.edit_text(
            f"ğŸ¦™ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼: {context_db[user_id]['model_name']}\n"
            f"âŒ ğ—”ğ—¹ğ—´ğ—¼ ğ—±ğ—²ğ˜‚ ğ—²ğ—¿ğ—¿ğ—¼, ğ˜ğ—²ğ—»ğ˜ğ—² ğ—»ğ—¼ğ˜ƒğ—®ğ—ºğ—²ğ—»ğ˜ğ—² ğ—ºğ—®ğ—¶ğ˜€ ğ˜ğ—®ğ—¿ğ—±ğ—²."
        )

    # response = client.fetch(
    #     url=f"{client.url}/models",
    #     method="POST",
    #     params=params,
    #     json={},
    #     headers={"content-type": "application/json"},
    # )
    #
    # if response["code"] != 2:
    #     return await callback_query.edit_message_text(
    #         f"ğŸ¦™ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼: {context_db[user_id]['model_name']}\n"
    #         f"âŒ ğ—”ğ—¹ğ—´ğ—¼ ğ—±ğ—²ğ˜‚ ğ—²ğ—¿ğ—¿ğ—¼, ğ˜ğ—²ğ—»ğ˜ğ—² ğ—»ğ—¼ğ˜ƒğ—®ğ—ºğ—²ğ—»ğ˜ğ—² ğ—ºğ—®ğ—¶ğ˜€ ğ˜ğ—®ğ—¿ğ—±ğ—²."
    #     )
    #
    # await callback_query.message.delete()
    # await callback_query.message.reply_to_message.reply_text(
    #     f"ğŸ“ <u>ğ—£ğ—¿ğ—¼ğ—ºğ—½ğ˜</u>: {context_db[user_id]['prompt']}"
    #     f"\nğŸ¦™ <u>ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼</u>: {context_db[user_id]['model_name']}"
    #     f"\nğŸ“¬ <u>ğ—¥ğ—²ğ˜€ğ—½ğ—¼ğ˜€ğ˜ğ—®</u>: \n\n"
    #     f"<i>{response['content']}</i>",
    #     reply_to_message_id=context_db[user_id]["reply_to_id"],
    # )


@app.on_callback_query(filters.regex(pattern=r"^llm_cancel_\d+") & ~BANNED_USERS)
async def cancel(_: Client, callback_query: CallbackQuery):
    user_id = int(callback_query.data.split("_")[2])

    if callback_query.from_user.id != user_id:
        return await callback_query.answer("âŒ ğ—¡Ã£ğ—¼ ğ—®ğ˜‚ğ˜ğ—¼ğ—¿ğ—¶ğ˜‡ğ—®ğ—±ğ—¼.", show_alert=True)

    del context_db[user_id]

    await callback_query.edit_message_text("ğŸ¦™ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¼ ğ—°ğ—®ğ—»ğ—°ğ—²ğ—¹ğ—®ğ—±ğ—¼.")


def normalize_username(username: str) -> str:
    normalized = unicodedata.normalize("NFKC", username)
    normalized = re.sub(r"\W+", "", normalized)
    return normalized


def get_prompt(message: Message) -> str:
    return message.text.split(None, 1)[1].strip() if len(message.text.split()) > 1 else (
        message.reply_to_message.text if message.reply_to_message else None
    )


__MODULE__ = "ğŸ¦™ ğ—Ÿğ—Ÿğ— "
__HELP__ = """
ğŸ› ï¸ ğ— ğ—¼Ìğ—±ğ˜‚ğ—¹ğ—¼ ğ—±ğ—² ğ—Ÿğ—¶ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—²ğ—º ğ—±ğ—² ğ—šğ—¿ğ—®ğ—»ğ—±ğ—² ğ—˜ğ˜€ğ—°ğ—®ğ—¹ğ—®

<b>ğŸ“ ğ——ğ—²ğ˜€ğ—°ğ—¿ğ—¶ğ—°Ì§ğ—®Ìƒğ—¼:</b>

- ğ—˜ğ˜€ğ˜ğ—² ğ—ºğ—¼Ìğ—±ğ˜‚ğ—¹ğ—¼ ğ—½ğ—²ğ—¿ğ—ºğ—¶ğ˜ğ—² ğ—¾ğ˜‚ğ—² ğ˜ƒğ—¼ğ—°ğ—²Ì‚ ğ˜‚ğ˜€ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ—¼ğ˜€ ğ—±ğ—² ğ—¹ğ—¶ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—²ğ—º ğ—±ğ—² ğ—´ğ—¿ğ—®ğ—»ğ—±ğ—² ğ—²ğ˜€ğ—°ğ—®ğ—¹ğ—® ğ—½ğ—®ğ—¿ğ—® ğ—´ğ—²ğ—¿ğ—®ğ—¿ ğ˜ğ—²ğ˜…ğ˜ğ—¼ ğ—°ğ—¼ğ—º ğ—¯ğ—®ğ˜€ğ—² ğ—²ğ—º ğ˜‚ğ—º ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜.

<b>ğŸ”– ğ—–ğ—¼ğ—ºğ—®ğ—»ğ—±ğ—¼ğ˜€:</b>

â€¢ <code>/llm</code> [ğ˜ğ—²ğ˜…ğ˜ğ—¼]: ğ—šğ—²ğ—¿ğ—² ğ˜ğ—²ğ˜…ğ˜ğ—¼ ğ—°ğ—¼ğ—º ğ—¯ğ—®ğ˜€ğ—² ğ—»ğ—¼ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—³ğ—¼ğ—¿ğ—»ğ—²ğ—°ğ—¶ğ—±ğ—¼.

â€¢ <code>/llmgen</code> [ğ˜ğ—²ğ˜…ğ˜ğ—¼]: ğ—šğ—²ğ—¿ğ—² ğ˜ğ—²ğ˜…ğ˜ğ—¼ ğ—°ğ—¼ğ—º ğ—¯ğ—®ğ˜€ğ—² ğ—»ğ—¼ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—³ğ—¼ğ—¿ğ—»ğ—²ğ—°ğ—¶ğ—±ğ—¼.

â€¢ <code>/llmgenerate</code> [ğ˜ğ—²ğ˜…ğ˜ğ—¼]: ğ—šğ—²ğ—¿ğ—² ğ˜ğ—²ğ˜…ğ˜ğ—¼ ğ—°ğ—¼ğ—º ğ—¯ğ—®ğ˜€ğ—² ğ—»ğ—¼ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—³ğ—¼ğ—¿ğ—»ğ—²ğ—°ğ—¶ğ—±ğ—¼.

â€¢ <code>/llma</code> [ğ˜ğ—²ğ˜…ğ˜ğ—¼]: ğ—šğ—²ğ—¿ğ—² ğ˜ğ—²ğ˜…ğ˜ğ—¼ ğ—°ğ—¼ğ—º ğ—¯ğ—®ğ˜€ğ—² ğ—»ğ—¼ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ ğ—³ğ—¼ğ—¿ğ—»ğ—²ğ—°ğ—¶ğ—±ğ—¼.

<b>ğŸ’¡ ğ—˜ğ˜…ğ—²ğ—ºğ—½ğ—¹ğ—¼ğ˜€:</b>

â€¢ <code>/llm</code> O que Ã© a vida?
â€¢ <code>/llmgen</code> O que Ã© a vida?
â€¢ <code>/llma</code> Como fazer um bolo?
"""
